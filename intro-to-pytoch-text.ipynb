{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Intro-to-Pytorch\" data-toc-modified-id=\"Intro-to-Pytorch-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Intro to Pytorch</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pytorch-tensors\" data-toc-modified-id=\"Pytorch-tensors-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Pytorch tensors</a></span></li><li><span><a href=\"#Pytorch-Autograd\" data-toc-modified-id=\"Pytorch-Autograd-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Pytorch Autograd</a></span></li><li><span><a href=\"#torch.nn-module\" data-toc-modified-id=\"torch.nn-module-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>torch.nn module</a></span></li></ul></li><li><span><a href=\"#Linear-Regression-with-Pytorch\" data-toc-modified-id=\"Linear-Regression-with-Pytorch-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Linear Regression with Pytorch</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gradient-Descent-with-Pytorch\" data-toc-modified-id=\"Gradient-Descent-with-Pytorch-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Gradient Descent with Pytorch</a></span></li><li><span><a href=\"#Simplified-GD-Loop\" data-toc-modified-id=\"Simplified-GD-Loop-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Simplified GD Loop</a></span></li></ul></li><li><span><a href=\"#Text-Classification\" data-toc-modified-id=\"Text-Classification-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Text Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Subjectivity-Dataset\" data-toc-modified-id=\"Subjectivity-Dataset-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Subjectivity Dataset</a></span></li><li><span><a href=\"#Tokenization\" data-toc-modified-id=\"Tokenization-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Tokenization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simple-Tokenization\" data-toc-modified-id=\"Simple-Tokenization-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Simple Tokenization</a></span></li><li><span><a href=\"#Much-better-tokenization-with-Spacy\" data-toc-modified-id=\"Much-better-tokenization-with-Spacy-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Much better tokenization with Spacy</a></span></li></ul></li><li><span><a href=\"#Split-dataset-in-train-and-test\" data-toc-modified-id=\"Split-dataset-in-train-and-test-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Split dataset in train and test</a></span></li><li><span><a href=\"#Word-to-index-mapping\" data-toc-modified-id=\"Word-to-index-mapping-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Word to index mapping</a></span></li><li><span><a href=\"#Sentence-encoding\" data-toc-modified-id=\"Sentence-encoding-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Sentence encoding</a></span></li><li><span><a href=\"#Embedding-layer\" data-toc-modified-id=\"Embedding-layer-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Embedding layer</a></span></li><li><span><a href=\"#Continuous-Bag-of-Words-Model\" data-toc-modified-id=\"Continuous-Bag-of-Words-Model-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Continuous Bag of Words Model</a></span></li></ul></li><li><span><a href=\"#Training-the-CBOW-model\" data-toc-modified-id=\"Training-the-CBOW-model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Training the CBOW model</a></span></li><li><span><a href=\"#Data-loaders-for-SGD\" data-toc-modified-id=\"Data-loaders-for-SGD-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Data loaders for SGD</a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch libraries\n",
    "%matplotlib inline\n",
    "import torch \n",
    "import torch.autograd as autograd \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch consists of 4 main packages:\n",
    "* torch: a general purpose array library similar to Numpy that can do computations on GPU\n",
    "* torch.autograd: a package for automatically obtaining gradients\n",
    "* torch.nn: a neural net library with common layers and cost functions\n",
    "* torch.optim: an optimization package with common optimization algorithms like SGD, Adam, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch tensors\n",
    "Like Numpy tensors but can utilize GPUs to accelerate its numerical computations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random tensor\n",
    "N = 5\n",
    "x = torch.randn(N, 10).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0747,  1.9776,  0.5117, -1.8066, -0.6831, -0.8641, -0.7846,\n",
       "         -0.5641, -0.3162,  0.0134],\n",
       "        [ 0.6028,  0.5037, -0.0264,  0.7784, -0.3412,  0.5273, -0.0187,\n",
       "          1.4525,  1.0655,  0.3104],\n",
       "        [-0.5442, -1.6714, -1.4433, -0.6055, -1.6199,  1.0122, -0.9893,\n",
       "          0.9249, -0.6500, -0.9461],\n",
       "        [ 1.5730, -0.5012, -0.6782, -0.4189,  0.4268, -2.3263,  0.3965,\n",
       "         -0.0179,  1.4582,  0.0723],\n",
       "        [-1.3951, -0.6643,  0.6952,  0.3881, -0.4933, -0.9207, -0.3398,\n",
       "         -0.3773,  0.0142,  0.1346]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0747,  1.9776,  0.5117, -1.8066, -0.6831, -0.8641, -0.7846,\n",
       "         -0.5641, -0.3162,  0.0134,  0.6028,  0.5037, -0.0264,  0.7784,\n",
       "         -0.3412,  0.5273, -0.0187,  1.4525,  1.0655,  0.3104, -0.5442,\n",
       "         -1.6714, -1.4433, -0.6055, -1.6199,  1.0122, -0.9893,  0.9249,\n",
       "         -0.6500, -0.9461,  1.5730, -0.5012, -0.6782, -0.4189,  0.4268,\n",
       "         -2.3263,  0.3965, -0.0179,  1.4582,  0.0723, -1.3951, -0.6643,\n",
       "          0.6952,  0.3881, -0.4933, -0.9207, -0.3398, -0.3773,  0.0142,\n",
       "          0.1346]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshaping of tensors using .view()\n",
    "x.view(1,-1) #-1 makes torch infer the second dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(1,-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Autograd\n",
    "The autograd package in PyTorch provides classes and functions implementing automatic differentiation of arbitrary scalar valued function. For example, the gradient of the error with respect to all parameters.\n",
    "\n",
    "In order for this to happen we need to declare our paramerers as Tensors with the requires_grad=True keyword. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3., 4., 5., 6.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(48.)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = (2*x+1).sum()\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.backward() # computes the grad of L with respect to x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.,  2.,  2.,  2.,  2.,  2.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn module\n",
    "A neural net library with common layers and cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear transformation of a Nx5 matrix into a Nx3 matrix, where N can be anything \n",
    "# (number of observations)\n",
    "D = 5 # number of input featutes\n",
    "M = 3 # neurons in the first hidden layer\n",
    "linear_map = nn.Linear(D, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.2679, -0.0364,  0.0634,  0.2004, -0.0615],\n",
       "         [-0.2279,  0.0529,  0.2251, -0.4097, -0.1362],\n",
       "         [ 0.1216, -0.3447, -0.1211, -0.3438,  0.1313]]), Parameter containing:\n",
       " tensor([ 0.1550,  0.2002,  0.2355])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters are initialized randomly\n",
    "[p for p in linear_map.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Linear Regression with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of linear regression is to fit a line to a set of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we generate some fake data\n",
    "def lin(a,b,x): return a*x+b\n",
    "\n",
    "def gen_fake_data(n, a, b):\n",
    "    x = np.random.uniform(0,1,n) \n",
    "    y = lin(a,b,x) + 0.1 * np.random.normal(0,3,n)\n",
    "    return x, y\n",
    "\n",
    "x, y = gen_fake_data(50, 3., 8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x,y, s=8); plt.xlabel(\"x\"); plt.ylabel(\"y\"); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to find **parameters** (weights) $a$ and $b$ such that you minimize the *error* between the points and the line $a\\cdot x + b$. Note that here $a$ and $b$ are unknown. For a regression problem the most common *error function* or *loss function* is the **mean squared error**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_hat, y): return ((y_hat - y) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we believe $a = 10$ and $b = 5$ then we can compute `y_hat` which is our *prediction* and then compute our error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.160666496399903"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = lin(10,5,x)\n",
    "mse(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(a, b, x, y): return mse(lin(a,b,x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.160666496399903"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss(10, 5, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have specified the *model* (linear regression) and the *evaluation criteria* (or *loss function*). Now we need to handle *optimization*; that is, how do we find the best values for $a$ and $b$? How do we find the best *fitting* linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a fixed dataset $x$ and $y$ `mse_loss(a,b)` is a function of $a$ and $b$. We would like to find the values of $a$ and $b$ that minimize that function.\n",
    "\n",
    "**Gradient descent** is an algorithm that minimizes functions. Given a function defined by a set of parameters, gradient descent starts with an initial set of parameter values and iteratively moves toward a set of parameter values that minimize the function. This iterative minimization is achieved by taking steps in the negative direction of the function gradient.\n",
    "\n",
    "Here is gradient descent implemented in [PyTorch](http://pytorch.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate some more data\n",
    "x, y = gen_fake_data(10000, 3., 8.)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap x and y as tensor \n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.9092], dtype=torch.float64),\n",
       " tensor([-1.2743], dtype=torch.float64))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create random Tensors for weights, and wrap them in tensors.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these tensors during the backward pass.\n",
    "a, b = np.random.randn(1), np.random.randn(1)\n",
    "a = torch.tensor(a, requires_grad=True)\n",
    "b = torch.tensor(b, requires_grad=True)\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139.564074491229\n",
      "0.9659864008960766\n",
      "0.09549616703384778\n",
      "0.08999920106099034\n",
      "0.08994163720791452\n",
      "0.08992354426309486\n",
      "0.08990978507799505\n",
      "0.08989919746750195\n",
      "0.08989104957575292\n",
      "0.0898847792097955\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "for t in range(10000):\n",
    "    # Forward pass: compute predicted y using operations on Variables\n",
    "    loss = mse_loss(a,b,x,y)\n",
    "    if t % 1000 == 0: print(loss.item())\n",
    "    \n",
    "    # Computes the gradient of loss with respect to all Variables with requires_grad=True.\n",
    "    # After this call a.grad and b.grad will be Variables holding the gradient\n",
    "    # of the loss with respect to a and b respectively\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update a and b using gradient descent; a.data and b.data are Tensors,\n",
    "    # a.grad and b.grad are Variables and a.grad.data and b.grad.data are Tensors\n",
    "    a.data -= learning_rate * a.grad.data\n",
    "    b.data -= learning_rate * b.grad.data\n",
    "    \n",
    "    # Zero the gradients\n",
    "    a.grad.data.zero_()\n",
    "    b.grad.data.zero_()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.9939], dtype=torch.float64) tensor([ 8.0065], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified GD Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1, out_features=1, bias=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear tranformation with input dimension=1 and output dimension=1\n",
    "nn.Linear(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple way of specifying a linear regression model\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(1, 1),\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent way of specifiying the same model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.lin = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.lin(x)\n",
    "        return x \n",
    "model =  LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.3880]]), Parameter containing:\n",
      "tensor([ 0.2672])]\n"
     ]
    }
   ],
   "source": [
    "print([p for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = gen_fake_data(10000, 3., 8.)\n",
    "x = torch.tensor(x).float()\n",
    "y = torch.tensor(y).float()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you have to be careful with the dimensions that your model is expecting\n",
    "x1 = torch.unsqueeze(x, 1)\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.3191e-02],\n",
      "        [ 1.7825e-02],\n",
      "        [ 1.7551e-01],\n",
      "        ...,\n",
      "        [ 5.5174e-02],\n",
      "        [ 3.8064e-02],\n",
      "        [ 1.8437e-01]])\n"
     ]
    }
   ],
   "source": [
    "y_hat = model(x1)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.881103515625\n",
      "0.09073980152606964\n",
      "0.09073547273874283\n",
      "0.09073548018932343\n",
      "0.09073548018932343\n",
      "0.09073545783758163\n",
      "0.09073545038700104\n",
      "0.09073545038700104\n",
      "0.09073542803525925\n",
      "0.09073539078235626\n"
     ]
    }
   ],
   "source": [
    "for t in range(10000):\n",
    "    # Forward pass: compute predicted y using operations on Variables\n",
    "    y_hat = model(x1)\n",
    "    loss = F.mse_loss(y_hat, y.unsqueeze(1))\n",
    "    if t % 1000 == 0: print(loss.item())\n",
    "       \n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 2.9936]]), Parameter containing:\n",
      "tensor([ 8.0003])]\n"
     ]
    }
   ],
   "source": [
    "print([p for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "In this part of the tutorial we develop a continuous bag of words (CBOW) model for a text classification task described [here]( https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf). The CBOW model was first described [here](https://arxiv.org/pdf/1301.3781.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subjectivity Dataset\n",
    "The subjectivity dataset has 5000 subjective and 5000 objective processed sentences. To get the data:\n",
    "```\n",
    "wget http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_dataset():\n",
    "    ! wget http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz\n",
    "    ! mkdir data\n",
    "    ! tar -xvf rotten_imdb.tar.gz -C data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpack_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot.tok.gt9.5000   quote.tok.gt9.5000  subjdata.README.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the movie begins in the past where a young boy named sam attempts to save celebi from a hunter . \r\n",
      "emerging from the human psyche and showing characteristics of abstract expressionism , minimalism and russian constructivism , graffiti removal has secured its place in the history of modern art while being created by artists who are unconscious of their artistic achievements . \r\n"
     ]
    }
   ],
   "source": [
    "! head -2 data/plot.tok.gt9.5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/plot.tok.gt9.5000'),\n",
       " PosixPath('data/subjdata.README.1.0'),\n",
       " PosixPath('data/quote.tok.gt9.5000')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "PATH = Path(\"data\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization is the task of chopping up text into pieces, called tokens.\n",
    "\n",
    "spaCy is an open-source software library for advanced Natural Language Processing. Here we will use it for tokenization.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need each line in the file \n",
    "def read_file(path):\n",
    "    \"\"\" Read file returns a list of lines.\n",
    "    \"\"\"\n",
    "    with open(path, encoding = \"ISO-8859-1\") as f:\n",
    "        content = f.readlines()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_lines = read_file(PATH/\"plot.tok.gt9.5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the movie begins in the past where a young boy named sam attempts to save celebi from a hunter . \\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the', 'movie', 'begins', 'in', 'the', 'past', 'where', 'a',\n",
       "       'young', 'boy', 'named', 'sam', 'attempts', 'to', 'save', 'celebi',\n",
       "       'from', 'a', 'hunter', '.'], dtype='<U8')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(obj_lines[0].strip().lower().split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Much better tokenization with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first time run this\n",
    "#!python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_lines = read_file(PATH/\"plot.tok.gt9.5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(obj_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tok(obj_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([x for x in test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_content = read_file(PATH/\"quote.tok.gt9.5000\")\n",
    "obj_content = read_file(PATH/\"plot.tok.gt9.5000\")\n",
    "sub_content = np.array([line.strip().lower() for line in sub_content])\n",
    "obj_content = np.array([line.strip().lower() for line in obj_content])\n",
    "sub_y = np.zeros(len(sub_content))\n",
    "obj_y = np.ones(len(obj_content))\n",
    "X = np.append(sub_content, obj_content)\n",
    "y = np.append(sub_y, obj_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('smart and alert , thirteen conversations about one thing is a small gem .',\n",
       " 0.0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['will god let her fall or give her a new path ?',\n",
       "        \"the director's twitchy sketchbook style and adroit perspective shifts grow wearisome amid leaden pacing and indifferent craftsmanship ( most notably wretched sound design ) .\",\n",
       "        \"welles groupie/scholar peter bogdanovich took a long time to do it , but he's finally provided his own broadside at publishing giant william randolph hearst .\",\n",
       "        'based on the 1997 john king novel of the same name with a rather odd synopsis : \" a first novel about a seasoned chelsea football club hooligan who represents a disaffected society operating by brutal rules .',\n",
       "        'yet , beneath an upbeat appearance , she is struggling desperately with the emotional and physical scars left by the attack .'],\n",
       "       dtype='<U691'), array([1., 0., 0., 1., 1.]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5], y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word to index mapping\n",
    "In interest of time we will tokenize without spaCy. Here we will compute a vocabulary of words based on the training set and a mapping from word to an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(content):\n",
    "    \"\"\"Computes Dict of counts of words.\n",
    "    \n",
    "    Computes the number of times a word is on a document.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(float)\n",
    "    for line in content:\n",
    "        words = set(line.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "    return vocab      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the vocabulary from the training set\n",
    "word_count = get_vocab(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21415"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's delete words that are very infrequent\n",
    "for word in list(word_count):\n",
    "    if word_count[word] < 5:\n",
    "        del word_count[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4065"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally we need an index for each word in the vocab\n",
    "vocab2index = {\"<PAD>\":0, \"UNK\":1} # init with padding and unknown\n",
    "words = [\"<PAD>\", \"UNK\"]\n",
    "for word in word_count:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab2index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence encoding\n",
    "Here we encode each sentence as a sequence of indices corresponding to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_len = np.array([len(x.split()) for x in X_train])\n",
    "x_test_len = np.array([len(x.split()) for x in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(x_train_len, 95) # let set the max sequence len to N=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'will god let her fall or give her a new path ?'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns the index of the word or the index of \"UNK\" otherwise\n",
    "vocab2index.get(\"?\", vocab2index[\"UNK\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12,  6,  9,  2, 10,  4,  3,  2,  8,  5, 11,  7])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in X_train[0].split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(s, N=40):\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in s.split()])\n",
    "    l = min(N, len(enc1))\n",
    "    enc[:l] = enc1[:l]\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12,  6,  9,  2, 10,  4,  3,  2,  8,  5, 11,  7,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0], dtype=int32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sentence(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 40)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.vstack([encode_sentence(x) for x in X_train])\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 40)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = np.vstack([encode_sentence(x) for x in X_test])\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layer\n",
    "Most deep learning models use a dense vectors of real numbers as representation of words (word embeddings), as opposed to a one-hot encoding representations. The module torch.nn.Embedding is used to represent word embeddings. It takes two arguments: the vocabulary size, and the dimensionality of the embeddings. The embeddings are initialized with random vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.8536,  0.1352, -0.2728,  0.4877],\n",
       "        [ 0.1901,  1.3434,  2.4007, -1.3473],\n",
       "        [ 1.9391,  0.4154,  1.8454, -0.0440],\n",
       "        [-0.8543,  1.9308, -0.4309, -0.5002],\n",
       "        [-1.2015,  0.6970, -0.4598,  0.1874],\n",
       "        [-0.5489,  1.1735, -1.6129, -0.3163],\n",
       "        [ 1.2211, -0.1239,  0.3409, -1.1177],\n",
       "        [ 0.5130, -1.4622,  0.4938,  0.4620],\n",
       "        [-0.0666,  1.7076, -2.7429, -0.1095]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an Embedding module containing 10 words with embedding size 4\n",
    "# embedding will be initialized at random\n",
    "embed = nn.Embedding(10, 4, padding_idx=0)\n",
    "embed.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `padding_idx` has embedding vector 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8536,  0.1352, -0.2728,  0.4877],\n",
       "         [-0.8543,  1.9308, -0.4309, -0.5002],\n",
       "         [ 0.8536,  0.1352, -0.2728,  0.4877],\n",
       "         [-1.2015,  0.6970, -0.4598,  0.1874],\n",
       "         [ 0.8536,  0.1352, -0.2728,  0.4877],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given a list of ids we can \"look up\" the embedding corresponing to each id\n",
    "# can you see that some vectors are the same?\n",
    "a = torch.LongTensor([[1,4,1,5,1,0]])\n",
    "embed(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would be the representation of a sentence with words with indices [1,4,1,5,1] and a padding at the end. Bellow we have an example in which we have two sentences. the first sentence has length 3 and the last sentence has length 2. In order to use a tensor we use padding at the end of the second sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.LongTensor([[1,4,1], [1,3,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model takes an average of the word embedding of each word. Here is how we do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.FloatTensor([3, 2]) # here is the size of the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8536,  0.1352, -0.2728,  0.4877],\n",
       "         [-0.8543,  1.9308, -0.4309, -0.5002],\n",
       "         [ 0.8536,  0.1352, -0.2728,  0.4877]],\n",
       "\n",
       "        [[ 0.8536,  0.1352, -0.2728,  0.4877],\n",
       "         [ 1.9391,  0.4154,  1.8454, -0.0440],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8529,  2.2012, -0.9765,  0.4752],\n",
       "        [ 2.7927,  0.5506,  1.5726,  0.4437]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(a).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2843,  0.7337, -0.3255,  0.1584],\n",
       "        [ 1.3964,  0.2753,  0.7863,  0.2219]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_embs = embed(a).sum(dim=1) \n",
    "sum_embs/ s.view(s.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size=100):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.word_emb = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_size, 1)\n",
    "        \n",
    "    def forward(self, x, s):\n",
    "        x = self.word_emb(x)\n",
    "        x = x.sum(dim=1)/ s.view(s.shape[0], 1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(vocab_size=5, emb_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.6823, -1.3827, -0.1621],\n",
       "        [-0.0323, -0.2556,  2.5525],\n",
       "        [-1.2646, -0.0038,  1.4429],\n",
       "        [-0.5084, -0.4949, -0.3695]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.word_emb.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5973],\n",
       "        [ 0.7232]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(a, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the CBOW model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067\n"
     ]
    }
   ],
   "source": [
    "V = len(words)\n",
    "model = CBOW(vocab_size=V, emb_size=50)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_metrics(model):\n",
    "    model.eval()\n",
    "    x = torch.LongTensor(x_test) #.cuda()\n",
    "    y = torch.Tensor(y_test).unsqueeze(1) #).cuda()\n",
    "    s = torch.Tensor(x_test_len).view(x_test_len.shape[0], 1)\n",
    "    y_hat = model(x, s)\n",
    "    loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "    y_pred = y_hat > 0\n",
    "    correct = (y_pred.float() == y).float().sum()\n",
    "    accuracy = correct/y_pred.shape[0]\n",
    "    print(\"test loss %.3f and accuracy %.3f\" % (loss.item(), accuracy.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss 0.702 and accuracy 0.477\n"
     ]
    }
   ],
   "source": [
    "# accuracy of a random model should be around 0.5\n",
    "test_metrics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, epochs=10, lr=0.01):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        x = torch.LongTensor(x_train)  #.cuda()\n",
    "        y = torch.Tensor(y_train).unsqueeze(1)\n",
    "        s = torch.Tensor(x_train_len).view(x_train_len.shape[0], 1)\n",
    "        y_hat = model(x, s)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.item())\n",
    "    test_metrics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7024547457695007\n",
      "0.6619945168495178\n",
      "0.5833789110183716\n",
      "0.4843262732028961\n",
      "0.37453681230545044\n",
      "0.2894887924194336\n",
      "0.2270020842552185\n",
      "0.1904212087392807\n",
      "0.16081099212169647\n",
      "0.14118488132953644\n",
      "test loss 0.264 and accuracy 0.902\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12197427451610565\n",
      "0.11815891414880753\n",
      "0.11501367390155792\n",
      "0.1116267591714859\n",
      "0.10820414870977402\n",
      "0.10499171167612076\n",
      "0.10200171917676926\n",
      "0.09909720718860626\n",
      "0.09619797766208649\n",
      "0.09334372729063034\n",
      "test loss 0.263 and accuracy 0.908\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=10, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epocs(model, epochs=10, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loaders for SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly all of deep learning is powered by one very important algorithm: **stochastic gradient descent (SGD)**. SGD can be seeing as an approximation of **gradient descent** (GD). In GD you have to run through *all* the samples in your training set to do a single itaration. In SGD you use *only one* or *a subset*  of training samples to do the update for a parameter in a particular iteration. The subset use in every iteration is called a **batch** or **minibatch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to create a data loader. The data loader provides the following features:\n",
    "* Batching the data\n",
    "* Shuffling the data\n",
    "* Load the data in parallel using multiprocessing workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence2(s, N=40):\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in s.split()])\n",
    "    l = min(N, len(enc1))\n",
    "    enc[:l] = enc1[:l]\n",
    "    return enc, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12,  6,  9,  2, 10,  4,  3,  2,  8,  5, 11,  7,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0], dtype=int32), 12)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sentence2(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubjectivityDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.x = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        x, s = encode_sentence2(x)\n",
    "        return x, self.y[idx], s\n",
    "    \n",
    "sub_dataset_train = SubjectivityDataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(sub_dataset_train, batch_size=5, shuffle=True)\n",
    "x, y, s = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  118,    16,  1273,  1359,    60,  3632,    60,    97,    28,\n",
       "            931,    64,     8,  1258,    30,    37,   497,  1035,     1,\n",
       "             18,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [    1,     1,     1,    17,     1,   475,   869,    60,     1,\n",
       "              1,    30,    60,    59,    82,    16,     1,    51,     1,\n",
       "              1,    28,   137,  2143,  2285,    18,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [    8,   128,   758,  1742,     1,  1834,     1,  3657,    17,\n",
       "              1,    18,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  824,    43,     8,   730,     1,    17,     8,     1,    51,\n",
       "              1,     1,     1,     1,    27,     1,    30,  1539,    30,\n",
       "              1,   215,    28,    90,     1,    30,    67,   105,   305,\n",
       "             18,    17,    95,  1716,     8,  2382,    79,    16,    89,\n",
       "             51,    16,     1,  1498],\n",
       "         [    1,   152,    16,  1250,  2118,    72,  3636,    30,    16,\n",
       "              1,    59,     1,    43,  4010,    18,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0]], dtype=torch.int32),\n",
       " tensor([ 1.,  1.,  0.,  0.,  1.], dtype=torch.float64),\n",
       " tensor([ 19,  24,  11,  40,  15]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(vocab_size=V, emb_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(sub_dataset_train, batch_size=500, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, epochs=10, lr=0.01):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        for x, y, s in train_loader:\n",
    "            x = x.type(torch.LongTensor)  #.cuda()\n",
    "            y = y.type(torch.FloatTensor).unsqueeze(1)\n",
    "            s = s.type(torch.Tensor).view(s.shape[0], 1)\n",
    "            y_hat = model(x, s)\n",
    "            loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(loss.item())\n",
    "    test_metrics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.595077395439148\n",
      "0.4045970141887665\n",
      "0.26646387577056885\n",
      "0.19933322072029114\n",
      "0.16005471348762512\n",
      "0.140223428606987\n",
      "0.12022583186626434\n",
      "0.11802416294813156\n",
      "0.08472650498151779\n",
      "0.08180411905050278\n",
      "0.08018757402896881\n",
      "0.08355165272951126\n",
      "0.06486327946186066\n",
      "0.05094480887055397\n",
      "0.051483701914548874\n",
      "0.04123803228139877\n",
      "0.03242563083767891\n",
      "0.02988939918577671\n",
      "0.028660159558057785\n",
      "0.025195850059390068\n",
      "test loss 0.336 and accuracy 0.890\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# References\n",
    "* https://pytorch.org/docs/stable/index.html\n",
    "* http://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "* https://hsaghir.github.io/data_science/pytorch_starter/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "nav_menu": {},
  "toc": {
   "nav_menu": {
    "height": "116px",
    "width": "251px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
